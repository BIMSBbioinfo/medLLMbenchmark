{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import boto3\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_display(data):\n",
    "    # Display the DataFrame with scroll and define the height and width for the scrollable area\n",
    "    display(HTML(f'''\n",
    "    <div style=\"height: 500px; overflow-y: scroll; overflow-x: scroll; border: 1px solid black; padding: 5px;\">\n",
    "        {data.to_html(max_rows=None, max_cols=None)}\n",
    "    </div>\n",
    "    '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: LLM prediction for General User Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to perform prediction with an LLM for the General User Case with retry logic\n",
    "def get_prediction_GeneralUser(row, chain, max_retries=5, initial_wait=1):\n",
    "    hpi = row['HPI']\n",
    "    patient_info = row[\"patient_info\"]\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Invoke the chain with the HPI and patient_info\n",
    "            response = chain.invoke({\"HPI\": hpi, \"patient_info\": patient_info}).content\n",
    "            return response  # Return on successful invocation\n",
    "\n",
    "        except Exception as e:\n",
    "            # Check if the error is a ThrottlingException or similar\n",
    "            if \"ThrottlingException\" in str(e) or \"Too many requests\" in str(e):\n",
    "                # Exponential backoff\n",
    "                wait_time = initial_wait * (2 ** attempt)\n",
    "                print(f\"Throttling detected. Retrying after {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                # Handle other types of exceptions\n",
    "                return f\"Error: {str(e)}\"\n",
    "    # If all retries fail, return an error\n",
    "    return \"Error: Max retries exceeded\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: LLM prediction for Clincial User Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to perform prediction with an LLM for the Clinical User Case with retry logic\n",
    "def get_prediction_ClinicalUser(row, chain, max_retries=5, initial_wait=1):\n",
    "    hpi = row['HPI']\n",
    "    patient_info = row[\"patient_info\"]\n",
    "    initial_vitals = row[\"initial_vitals\"]\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            # Invoke the chain with the HPI, patient_info and initial_vitals\n",
    "            response = chain.invoke({\"hpi\": hpi, \"patient_info\": patient_info, \"initial_vitals\": initial_vitals}).content\n",
    "            return response  # Return on successful invocation\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Check if the error is a ThrottlingException or similar\n",
    "            if \"ThrottlingException\" in str(e) or \"Too many requests\" in str(e):\n",
    "                # Exponential backoff\n",
    "                wait_time = initial_wait * (2 ** attempt)\n",
    "                print(f\"Throttling detected. Retrying after {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                # Handle other types of exceptions\n",
    "                return f\"Error: {str(e)}\"\n",
    "    # If all retries fail, return an error\n",
    "    return \"Error: Max retries exceeded\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prediction Specialty and Diagnosis General User Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data from create_ground_truth_specialty.ipynb\n",
    "df = pd.read_csv(\"MIMIC-IV-Ext-Diagnosis-Specialty.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the prompt template\n",
    "prompt = \"\"\"You are an experienced healthcare professional with expertise in determining the medical specialty and diagnosis based on a patient's history of present illness and personal information. Review the data and identify the three most likely, distinct specialties to manage the condition, followed by the three most likely diagnoses. List specialties first, in order of likelihood, then diagnoses. \n",
    "Respond with the specialties in <specialty> tags and the diagnoses in <diagnosis> tags.\n",
    "History of present illness: {hpi} and personal information: {patient_info}.\"\"\"\n",
    "\n",
    "\n",
    "## set AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"Enter your AWS Access Key ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"Enter your AWS Secret Access Key\"\n",
    "\n",
    "prompt_chain = PromptTemplate(template=prompt,input_variables=[\"hpi\", \"patient_info\"])\n",
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=str(\"us-east-1\"))\n",
    "\n",
    "\n",
    "## Claude Sonnet 3.5\n",
    "llm_claude35 = ChatBedrock(model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude35 = prompt_chain | llm_claude35\n",
    "\n",
    "\n",
    "## Claude Sonnet 3\n",
    "llm_claude3 = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude3 = prompt_chain | llm_claude3\n",
    "\n",
    "\n",
    "## Claude 3 Haiku\n",
    "llm_haiku = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_haiku = prompt_chain | llm_haiku\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df['diag_spec_Claude3.5'] = df.progress_apply(lambda row: get_prediction_GeneralUser(row, chain_claude35), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Diagnosis-Specialty.csv', index=False)\n",
    "\n",
    "df['diag_spec_Claude3'] = df.progress_apply(lambda row: get_prediction_GeneralUser(row, chain_claude3), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Diagnosis-Specialty.csv', index=False)\n",
    "\n",
    "df['diag_spec_Haiku'] = df.progress_apply(lambda row: get_prediction_GeneralUser(row, chain_haiku), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Diagnosis-Specialty.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prediction Specialty and Diagnosis Clinical User Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the prompt template\n",
    "prompt = \"\"\"You are an experienced healthcare professional with expertise in determining the medical specialty and diagnosis based on a patient's history of present illness, personal information and initial vitals. Review the data and identify the three most likely, distinct specialties to manage the condition, followed by the three most likely diagnoses. List specialties first, in order of likelihood, then diagnoses.\n",
    "Respond with the specialties in <specialty> tags and the diagnoses in <diagnosis> tags.\n",
    "History of present illness: {hpi}, personal information: {patient_info} and initial vitals: {initial_vitals}.\"\"\"\n",
    "\n",
    "\n",
    "## set AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"Enter your AWS Access Key ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"Enter your AWS Secret Access Key\"\n",
    "\n",
    "prompt_chain = PromptTemplate(template=prompt,input_variables=[\"hpi\", \"patient_info\", \"initial_vitals\"])\n",
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=str(\"us-east-1\"))\n",
    "\n",
    "\n",
    "## Claude Sonnet 3.5\n",
    "llm_claude35 = ChatBedrock(model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude35 = prompt_chain | llm_claude35\n",
    "\n",
    "\n",
    "## Claude Sonnet 3\n",
    "llm_claude3 = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude3 = prompt_chain | llm_claude3\n",
    "\n",
    "\n",
    "## Claude 3 Haiku\n",
    "llm_haiku = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_haiku = prompt_chain | llm_haiku\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df['diag_spec_Claude3.5_Clinical'] = df.progress_apply(lambda row: get_prediction_ClinicalUser(row, chain_claude35), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Diagnosis-Specialty.csv', index=False)\n",
    "\n",
    "df['diag_spec_Claude3_Clinical'] = df.progress_apply(lambda row: get_prediction_ClinicalUser(row, chain_claude3), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Diagnosis-Specialty.csv', index=False)\n",
    "\n",
    "df['diag_spec_Haiku_Clinical'] = df.progress_apply(lambda row: get_prediction_ClinicalUser(row, chain_haiku), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Diagnosis-Specialty.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prediction Triage General User Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Data from create_ground_truth_specialty.py\n",
    "df = pd.read_csv(\"MIMIC-IV-Ext-Triage.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the prompt template\n",
    "prompt = \"\"\"You are a nurse with emergency and triage experience. Using the patient's history of present illness and his information, determine the triage level based on the Emergency Severity Index (ESI), ranging from ESI level 1 (highest acuity) to ESI level 5 (lowest acuity): 1: Assign if the patient requires immediate lifesaving intervention. 2: Assign if the patient is in a high-risk situation (e.g., confused, lethargic, disoriented, or experiencing severe pain/distress)  3: Assign if the patient requires two or more diagnostic or therapeutic interventions and their vital signs are within acceptable limits for non-urgent care. 4: Assign if the patient requires one diagnostic or therapeutic intervention (e.g., lab test, imaging, or EKG). 5: Assign if the patient does not require any diagnostic or therapeutic interventions beyond a physical exam (e.g., no labs, imaging, or wound care).\n",
    "History of present illness: {HPI} and patient info: {patient_info}. Respond with the level in an <acuity> tag.\"\"\"\n",
    "\n",
    "\n",
    "## set AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"Enter your AWS Access Key ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"Enter your AWS Secret Access Key\"\n",
    "\n",
    "prompt_chain = PromptTemplate(template=prompt,input_variables=[\"hpi\", \"patient_info\"])\n",
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=str(\"us-east-1\"))\n",
    "\n",
    "\n",
    "## Claude Sonnet 3.5\n",
    "llm_claude35 = ChatBedrock(model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude35 = prompt_chain | llm_claude35\n",
    "\n",
    "\n",
    "## Claude Sonnet 3\n",
    "llm_claude3 = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude3 = prompt_chain | llm_claude3\n",
    "\n",
    "\n",
    "## Claude 3 Haiku\n",
    "llm_haiku = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_haiku = prompt_chain | llm_haiku\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df['triage_Claude3.5'] = df.progress_apply(lambda row: get_prediction_GeneralUser(row, chain_claude35), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Triage.csv', index=False)\n",
    "\n",
    "df['triage_Claude3'] = df.progress_apply(lambda row: get_prediction_GeneralUser(row, chain_claude3), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Triage.csv', index=False)\n",
    "\n",
    "df['triage_Haiku'] = df.progress_apply(lambda row: get_prediction_GeneralUser(row, chain_haiku), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Triage.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Prediction Triage Clinical User Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the prompt template\n",
    "prompt = \"\"\"You are a nurse with emergency and triage experience. Using the patient's history of present illness, his information and initial vitals, determine the triage level based on the Emergency Severity Index (ESI), ranging from ESI level 1 (highest acuity) to ESI level 5 (lowest acuity): 1: Assign if the patient requires immediate lifesaving intervention. 2: Assign if the patient is in a high-risk situation (e.g., confused, lethargic, disoriented, or experiencing severe pain/distress)  3: Assign if the patient requires two or more diagnostic or therapeutic interventions and their vital signs are within acceptable limits for non-urgent care. 4: Assign if the patient requires one diagnostic or therapeutic intervention (e.g., lab test, imaging, or EKG). 5: Assign if the patient does not require any diagnostic or therapeutic interventions beyond a physical exam (e.g., no labs, imaging, or wound care).\n",
    "History of present illness: {HPI}, patient info: {patient_info} and initial vitals: {initial_vitals}. Respond with the level in an <acuity> tag.\"\"\"\n",
    "\n",
    "\n",
    "## set AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"]=\"Enter your AWS Access Key ID\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"]=\"Enter your AWS Secret Access Key\"\n",
    "\n",
    "prompt_chain = PromptTemplate(template=prompt,input_variables=[\"hpi\", \"patient_info\", \"initial_vitals\"])\n",
    "client = boto3.client(service_name=\"bedrock-runtime\", region_name=str(\"us-east-1\"))\n",
    "\n",
    "\n",
    "## Claude Sonnet 3.5\n",
    "llm_claude35 = ChatBedrock(model_id=\"anthropic.claude-3-5-sonnet-20240620-v1:0\", model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude35 = prompt_chain | llm_claude35\n",
    "\n",
    "\n",
    "## Claude Sonnet 3\n",
    "llm_claude3 = ChatBedrock(model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_claude3 = prompt_chain | llm_claude3\n",
    "\n",
    "\n",
    "## Claude 3 Haiku\n",
    "llm_haiku = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",  model_kwargs={\"temperature\": 0},client=client)\n",
    "chain_haiku = prompt_chain | llm_haiku\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "df['triage_Claude3.5_Clinical'] = df.progress_apply(lambda row: get_prediction_ClinicalUser(row, chain_claude35), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Triage.csv', index=False)\n",
    "\n",
    "df['triage_Claude3_Clinical'] = df.progress_apply(lambda row: get_prediction_ClinicalUser(row, chain_claude3), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Triage.csv', index=False)\n",
    "\n",
    "df['triage_Haiku_Clinical'] = df.progress_apply(lambda row: get_prediction_ClinicalUser(row, chain_haiku), axis=1)\n",
    "df.to_csv('MIMIC-IV-Ext-Triage.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envphd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
