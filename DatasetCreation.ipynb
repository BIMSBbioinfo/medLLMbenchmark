{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## README"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing MIMIC-IV\n",
    "resulted keys:  \n",
    "'stay_id', 'subject_id', 'hadm_id', 'patient_info', 'initial_vitals', 'pain', 'chiefcomplaint', 'tests', 'acuity', 'icd_code', 'icd_title', 'icd_version', 'HPI', 'diagnosis', 'primary_diagnosis', 'secondary_diagnosis'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_display(data):\n",
    "    # Display the DataFrame with scroll and define the height and width for the scrollable area\n",
    "    display(HTML(f'''\n",
    "    <div style=\"height: 500px; overflow-y: scroll; overflow-x: scroll; border: 1px solid black; padding: 5px;\">\n",
    "        {data.to_html(max_rows=None, max_cols=None)}\n",
    "    </div>\n",
    "    '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets from MIMIC-IV, MIMIC-IV-ED, MIMIC-IV-Note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load from MIMIC-IV-ED\n",
    "triage = pd.read_csv(\"/data/local/llm-evaluation/mimic-iv-ed-2.2/ed/triage.csv\", on_bad_lines='skip', low_memory=False)\n",
    "vitalsigns = pd.read_csv(\"/data/local/llm-evaluation/mimic-iv-ed-2.2/ed/vitalsign.csv\", on_bad_lines='skip', low_memory=False)\n",
    "ed_stays = pd.read_csv(\"/data/local/llm-evaluation/mimic-iv-ed-2.2/ed/edstays.csv\")\n",
    "diagnostics = pd.read_csv('/data/local/llm-evaluation/mimic-iv-ed-2.2/ed/diagnosis.csv',on_bad_lines='skip')\n",
    "\n",
    "#### Load from MIMIC-IV\n",
    "patients = pd.read_csv(\"/data/local/llm-evaluation/mimic-iv/mimic-iv-3.0/hosp/patients.csv.gz\", compression='gzip', low_memory=False)\n",
    "\n",
    "#### Load Discharge from MIMIC-IV-Note\n",
    "# Read the discharge.csv file into a string\n",
    "txt = open('/data/local/llm-evaluation/mimic-iv-note/discharge.csv').read()\n",
    "\n",
    "# Replace all occurrences of '|' with ',<vl>' (custom delimiter), ',\"\"\"\"\\n' with ',<br>' (indicating a line break marker), 'Followup Instructions:\\n___\\n\"\"\"\"' with new markers '</br>|' for parsing\n",
    "txt = txt.replace('|', ',<vl>')\n",
    "txt = txt.replace(',\"\"\"\"\\n', ',<br>')\n",
    "txt = txt.replace('Followup Instructions:\\n___\\n\"\"\"\"','Followup Instructions:\\n___\\n</br>|')\n",
    "\n",
    "# find text between <br> and </br> and replace any ',' with '<comma>'\n",
    "txt = re.sub(r'<br>([^<]*)</br>', lambda x: x.group(0).replace(',', '<comma>'), txt)\n",
    "\n",
    "# Remove all occurrences of double quotes '\"' from the text\n",
    "txt = txt.replace('\"', '')\n",
    "\n",
    "# Replace the 'text\\n' pattern with 'text|' to format for CSV parsing\n",
    "txt = txt.replace('text\\n', 'text|')\n",
    "\n",
    "# Use pandas to read the modified txt content as a CSV, using '|' as the line terminator\n",
    "df = pd.read_csv(StringIO(txt), lineterminator='|', on_bad_lines='warn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Add \"stay_id\" and \"text\" from edstays dataset\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        hadm_id = float(row['hadm_id'])\n",
    "        # Find the corresponding 'stay_id' in 'ed_stays' DataFrame that matches the 'hadm_id'\n",
    "        stay_id = ed_stays[ed_stays['hadm_id'] == hadm_id]['stay_id']\n",
    "\n",
    "        # If no matching 'stay_id' is found, skip to the next iteration\n",
    "        if stay_id.empty:\n",
    "            continue\n",
    "\n",
    "        df.at[index, 'stay_id'] = stay_id.iloc[0]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{e} at {index}\")\n",
    "        continue\n",
    "\n",
    "df = df[df['stay_id'].notnull()]\n",
    "\n",
    "#### Add all columns from Triage dataset\n",
    "# merged to the triage df, because it is unique on stay_id\n",
    "df = pd.merge(triage, df, on=\"stay_id\", how=\"inner\")\n",
    "\n",
    "#### Add gender and race from edstays dataset\n",
    "df = pd.merge(df, ed_stays, on='stay_id')\n",
    "\n",
    "## Removing Duplicate Rows Based on subject_id\n",
    "unique_df = df.drop_duplicates(subset=['subject_id_x'])\n",
    "\n",
    "#### Add age from patient dataset\n",
    "unique_df = pd.merge(unique_df, patients, on='subject_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Relevant Information from the Clinical Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract: Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tests(text):\n",
    "    lower_text = text.lower()\n",
    "    try:\n",
    "        if \"discharge labs\" in lower_text.split(\"pertinent results:\")[1].split('brief hospital course:')[0]:\n",
    "            return lower_text.split(\"pertinent results:\")[1].split('brief hospital course:')[0].split('discharge labs')[0]\n",
    "        else:\n",
    "            return lower_text.split(\"pertinent results:\")[1].split('brief hospital course:')[0]\n",
    "    except:\n",
    "        #print(lower_text)\n",
    "        return None\n",
    "\n",
    "unique_df[\"tests\"] = unique_df['text'].apply(get_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract: Past Medications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_medication(text):\n",
    "    lower_text = text.lower()\n",
    "    try:\n",
    "        # Extract the text between \"medications on admission:\" and \"discharge medications:\"\n",
    "        return lower_text.split(\"medications on admission:\")[1].split('discharge medications:')[0]\n",
    "    except:\n",
    "        # print(lower_text)\n",
    "        return None\n",
    "\n",
    "unique_df[\"past_medication\"] = unique_df['text'].apply(get_medication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract: History of Present Illness (to be continued and refined later on in the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HPI(text):\n",
    "    # Replace custom placeholders with their intended characters and clean up text markers\n",
    "    text = text.replace('<comma>', ',').replace('<br>', '').replace('</br>', '')\n",
    "    \n",
    "    # Extract the text between \"History of Present Illness:\" and \"Physical Exam:\" sections\n",
    "    text = text.split('History of Present Illness:')[-1].split('Physical Exam:')[0]\n",
    "    return text\n",
    "\n",
    "unique_df['preprocessed_text'] = unique_df['text'].apply(get_HPI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and organizing the DataFrame for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop Redundant Columns and Rename Relevant Columns for Consistency and Clarity\n",
    "unique_df = unique_df.drop(columns=['subject_id_x', 'subject_id_y', 'hadm_id_x', 'gender_y'])\n",
    "unique_df = unique_df.rename(columns={\n",
    "    'hadm_id_y': 'hadm_id',\n",
    "    'gender_x': 'gender',\n",
    "    'symptoms': 'preprocessed_text'\n",
    "})\n",
    "df = unique_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge ICD information from diagnostics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the diagnostics data to keep only rows where \"seq_num\" equals 1 (indicating the most relevant ICD code)\n",
    "diagnostics = diagnostics[diagnostics[\"seq_num\"] == 1]\n",
    "\n",
    "# Merge diagnostics data into df to add 'icd_code', 'icd_title', and 'icd_version' columns\n",
    "df = df.merge(diagnostics[['stay_id', 'icd_code', 'icd_title', \"icd_version\"]],\n",
    "              on='stay_id', how='left')\n",
    "\n",
    "# Remove rows where 'icd_code' is NaN\n",
    "df = df.dropna(subset=['icd_code'])\n",
    "\n",
    "# Drop columns that are no longer needed for the analysis or further processing\n",
    "df = df.drop(columns=[\"note_id\", \"note_type\", \"note_seq\", \"charttime\", \"storetime\", \"intime\", \"outtime\", \"arrival_transport\", \"disposition\", \"anchor_year\", \"anchor_year_group\", \"dod\" ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Initial Vitals from Temperature, Heartrate, respiration rate, o2 saturation, bloodpressure (dbp, sbp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vitals(row):\n",
    "    vitals = []\n",
    "    \n",
    "    if not pd.isna(row['temperature']):\n",
    "        vitals.append(f\"Temperature: {row['temperature']}\")\n",
    "    if not pd.isna(row['heartrate']):\n",
    "        vitals.append(f\"Heartrate: {row['heartrate']}\")\n",
    "    if not pd.isna(row['resprate']):\n",
    "        vitals.append(f\"resprate: {row['resprate']}\")\n",
    "    if not pd.isna(row['o2sat']):\n",
    "        vitals.append(f\"o2sat: {row['o2sat']}\")\n",
    "    if not pd.isna(row['sbp']):\n",
    "        vitals.append(f\"sbp: {row['sbp']}\")   \n",
    "    if not pd.isna(row['dbp']):\n",
    "        vitals.append(f\"dbp: {row['dbp']}\") \n",
    "    \n",
    "    return \", \".join(vitals)\n",
    "\n",
    "df.loc[:,'initial_vitals'] = df.apply(create_vitals, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Patient Info from Gender, Race and Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_info(row):\n",
    "    patient_info = []\n",
    "    \n",
    "   # Append the gender information with a readable format\n",
    "    if row[\"gender\"] == \"F\":\n",
    "        patient_info.append(\"Gender: Female\")\n",
    "    elif row[\"gender\"] == \"M\":\n",
    "        patient_info.append(\"Gender: Male\")\n",
    "    else:\n",
    "        patient_info.append(f\"Gender: {row['gender']}\")\n",
    "\n",
    "    patient_info.append(f\"Race: {row['race']}\")\n",
    "    patient_info.append(f\"Age: {row['anchor_age']}\")\n",
    "    \n",
    "    return \", \".join(patient_info)\n",
    "\n",
    "df.loc[:,'patient_info'] = df.apply(create_patient_info, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and organizing the DataFrame for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are no longer needed for the analysis or further processing and rearrange columns\n",
    "df = df.drop(columns=[\"gender\", \"race\", \"anchor_age\", \"temperature\", \"heartrate\", \"resprate\", \"o2sat\", \"sbp\", \"dbp\"])\n",
    "\n",
    "df = df[['stay_id', 'subject_id', 'hadm_id', \"text\", 'patient_info', 'initial_vitals', 'pain', 'chiefcomplaint', 'preprocessed_text', 'past_medication', 'tests', 'acuity', 'icd_code', 'icd_title', 'icd_version']]\n",
    "\n",
    "## remove rows that have nans in acuity, because acuity will be predicted and NaNs carry no useful information\n",
    "df = df.dropna(subset=['acuity'])\n",
    "df = df.dropna(subset=['tests'])\n",
    "\n",
    "## convert nans to empty strings\n",
    "df[\"pain\"] = df['pain'].fillna(\"\")\n",
    "df[\"chiefcomplaint\"] = df['chiefcomplaint'].fillna(\"\")\n",
    "df[\"past_medication\"] = df['past_medication'].fillna(\"\")\n",
    "\n",
    "## convert numpy.float64 to numpy.int64\n",
    "df['acuity'] = df['acuity'].astype(np.int64)\n",
    "df['hadm_id'] = df['hadm_id'].astype(np.int64)\n",
    "df['icd_version'] = df['icd_version'].astype(np.int64)\n",
    "\n",
    "## find the rows that have \"history of present illness\" in the \"text\" column and keep only these rows\n",
    "hpi = df['text'].str.contains('history of present illness', case=False, na=False)\n",
    "hpi_index = hpi[hpi==True].index\n",
    "df = df.loc[hpi_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract: HPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hpi(text):\n",
    "    pos_past_med_hist = text.lower().find('past medical history:')\n",
    "    pos_soc_hist = text.lower().find('social history:')\n",
    "    pos_fam_hist = text.lower().find('family history:')\n",
    "    #text = text.replace(\"\\n\", \" \")\n",
    "    if pos_past_med_hist != -1:\n",
    "        return text[:pos_past_med_hist].strip()\n",
    "    elif pos_soc_hist != -1:\n",
    "        return text[:pos_soc_hist].strip()\n",
    "    elif pos_soc_hist != -1:\n",
    "        return text[:pos_fam_hist].strip()\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "df[\"HPI\"] = df[\"preprocessed_text\"].apply(extract_hpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract: Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_diagnosis(text):\n",
    "    split_text = text.split(\"Discharge Diagnosis:\" )[-1].split(\"Discharge Condition:\")[0]\n",
    "    split_text= split_text.replace('<comma>', ', ')\n",
    "    return(\"Discharge Diagnosis: \" + split_text)\n",
    "\n",
    "df[\"diagnosis\"] = df[\"text\"].apply(extract_diagnosis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process HPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cut length of HPI <2000 and the tests <3000\n",
    "string_lengths = df['HPI'].str.len()\n",
    "mask = string_lengths<2000\n",
    "df = df[mask]\n",
    "\n",
    "string_lengths = df['HPI'].str.len()\n",
    "mask = string_lengths>50\n",
    "df = df[mask]\n",
    "\n",
    "string_lengths = df['tests'].str.len()\n",
    "mask = string_lengths<3000\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 477/18067 [00:00<00:03, 4764.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18067/18067 [00:03<00:00, 4638.82it/s]\n"
     ]
    }
   ],
   "source": [
    "## Removing Unwanted Sections Related to ED Course and Initial Vitals\n",
    "df = df.dropna(subset=['HPI'])\n",
    "df = df[df['HPI'] != \"\"]\n",
    "\n",
    "## HPI preprocess\n",
    "def extract_only_hpi(text):\n",
    "\n",
    "    ## remove everything after\n",
    "    #text = re.sub(re.compile(\"in the ED.*\", re.IGNORECASE), \"\", text)\n",
    "    text = re.sub(re.compile(r\"in the ED, initial vital.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    text = re.sub(re.compile(r\"in the ED initial vital.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    text = re.sub(re.compile(r\"\\bED Course.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    text = re.sub(re.compile(r\"\\bIn ED initial VS.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    text = re.sub(re.compile(r\"in the ED, initial VS.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    text = re.sub(re.compile(r\"\\binitial VS.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    text = re.sub(re.compile(r\"in the ED.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"HPI\"] = df[\"HPI\"].progress_apply(extract_only_hpi)\n",
    "\n",
    "## Remove the ones that have ED in them\n",
    "mask = df[\"HPI\"].str.contains(r'\\bED', case=False, na=False)\n",
    "df = df[~mask]\n",
    "## remove where test is nan to be able to compare between normal user and expert\n",
    "df = df.dropna(subset=['tests'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Specific Headers, Unwanted Sections, and Irrelevant Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove the header \"discharge diagnosis\"\n",
    "def remove_header(text, header):\n",
    "    text = re.sub(re.compile(header, re.IGNORECASE), \"\", text)\n",
    "    return text\n",
    "## Remove Header in diagnosis \"discharge diagnosis\"\n",
    "df['diagnosis'] = df['diagnosis'].apply(lambda text: remove_header(text, \"discharge diagnosis:\"))\n",
    "\n",
    "\n",
    "## Remove all content before and including the \"Facility:\\n___\" marker\n",
    "def delete_before_string(text):\n",
    "    text = re.sub(re.compile(r\".*Facility:\\n___\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    return text\n",
    "df['diagnosis'] = df['diagnosis'].apply(delete_before_string)\n",
    "\n",
    "\n",
    "## Remove all content before and including the \"___ Diagnosis:\" marker\n",
    "def delete_before_string(text):\n",
    "    text = re.sub(re.compile(r\".*___ Diagnosis:\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    return text\n",
    "df['diagnosis'] = df['diagnosis'].apply(delete_before_string)\n",
    "\n",
    "\n",
    "## Remove all content after the \"PMH\" marker (Past Medical History)\n",
    "def delete_after_string(text):\n",
    "    text = re.sub(re.compile(r\"PMH.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    return text\n",
    "df['diagnosis'] = df['diagnosis'].apply(delete_after_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter Rows with Excessive Information to Preserve Prediction Integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9299\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows in 'HPI' that contain specific terms like 'ER', 'Emergency room', 'Emergency department', or 'impression'\n",
    "# These rows likely refer to emergency settings and shouldn't be in the text for further analysis\n",
    "\n",
    "mask = df[\"HPI\"].str.contains(' ER ', case=False, na=False)\n",
    "df = df[~mask]\n",
    "mask = df[\"HPI\"].str.contains('Emergency room', case=False, na=False)\n",
    "df = df[~mask]\n",
    "mask = df[\"HPI\"].str.contains('Emergency department', case=False, na=False)\n",
    "df = df[~mask]\n",
    "mask = df[\"HPI\"].str.contains('impression', case=False, na=False)\n",
    "df = df[~mask]\n",
    "\n",
    "# Filter out rows in 'diagnosis' that contain the terms 'deceased' or 'died'\n",
    "mask = df[\"diagnosis\"].str.contains('deceased', case=False, na=False)\n",
    "df = df[~mask]\n",
    "mask = df[\"diagnosis\"].str.contains('died', case=False, na=False)\n",
    "df = df[~mask]\n",
    "\n",
    "# Further filter out rows where 'diagnosis' contains the term 'history of present illness'\n",
    "# This ensures that diagnosis-related fields don't inadvertently contain HPI-related content\n",
    "mask_hpi = df[\"diagnosis\"].str.contains('history of present illness', case=False, na=False)\n",
    "df = df[~mask_hpi]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Primary and Secondary Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop rows that include \"primary\" as primary diagnosis but not surely in the beginning\n",
    "mask = df[\"diagnosis\"].str.contains('primary', case=False, na=False)\n",
    "ind = df[mask].index.tolist()\n",
    "mask2 = df['diagnosis'].str.contains(r'^\\s*\\nprimary', flags=re.IGNORECASE, regex=True)\n",
    "ind2 = df[mask2].index.tolist()\n",
    "ind_drop = set(ind) - set(ind2)\n",
    "df = df[~df.index.isin(ind_drop)]\n",
    "\n",
    "## Drop rows that include \"secondary\" as secondary diagnosis but not surely in the beginning\n",
    "mask = df[\"diagnosis\"].str.contains('secondary', case=False, na=False)\n",
    "ind = df[mask].index.tolist()\n",
    "mask2 = df['diagnosis'].str.contains('\\nsecondary', flags=re.IGNORECASE, regex=True)\n",
    "ind2 = df[mask2].index.tolist()\n",
    "ind_drop = set(ind) - set(ind2)\n",
    "df = df[~df.index.isin(ind_drop)]\n",
    "\n",
    "\n",
    "## Segregate Discharge Diagnosis into Primary and Secondary Categories with Post-Processing and Filtering \n",
    "df[\"primary_diagnosis\"] = None\n",
    "df[\"secondary_diagnosis\"] = None\n",
    "## divide discharge diagnosis into primary and secondary diangosis if possible\n",
    "for i in df.index:\n",
    "    index = df[\"diagnosis\"][i].lower().find('secondary')\n",
    "    if index != -1:\n",
    "        df.loc[i, \"primary_diagnosis\"] = df[\"diagnosis\"][i][:index]\n",
    "        df.loc[i, \"secondary_diagnosis\"] = df[\"diagnosis\"][i][index:]\n",
    "    else:\n",
    "        df.loc[i, \"primary_diagnosis\"] = df[\"diagnosis\"][i]\n",
    "        df.loc[i, \"secondary_diagnosis\"] = \"\"\n",
    "\n",
    "\n",
    "# Remove any text after \"___ Condition:\" \n",
    "def delete_after_string(text):\n",
    "    text = re.sub(re.compile(r\"___ Condition:.*\", re.IGNORECASE | re.DOTALL), \"\", text)\n",
    "    return text\n",
    "df['primary_diagnosis'] = df['primary_diagnosis'].apply(delete_after_string)\n",
    "\n",
    "\n",
    "\n",
    "## Filter rows in the DataFrame where 'primary_diagnosis' has fewer than 16 single newlines (less than 16 diagnoses)\n",
    "def count_single_newlines(text):\n",
    "    single_newlines = re.findall(r'(?<!\\n)\\n(?!\\n)', text)\n",
    "    return len(single_newlines)\n",
    "\n",
    "# Apply the function to the entire column and get a list of counts\n",
    "newline_counts = df['primary_diagnosis'].apply(count_single_newlines).tolist()\n",
    "\n",
    "mask = [value < 16 for value in newline_counts]\n",
    "df = df[mask]\n",
    "\n",
    "df = df.drop(columns=['text', 'preprocessed_text', 'past_medication'], inplace=False)\n",
    "\n",
    "## Extract the first 2200\n",
    "df = df[:2200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Primary and Secondary Diagnosis into a list of diagnoses for each patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace colon without \\n to colon with \\n\n",
    "def colon_replacement(text):\n",
    "\n",
    "    # remove everything after\n",
    "    text = re.sub(r\":\\s*(?!\\n)\", ':\\n', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "df['primary_diagnosis'] = df['primary_diagnosis'].apply(colon_replacement)\n",
    "df['secondary_diagnosis'] = df['secondary_diagnosis'].apply(colon_replacement)\n",
    "\n",
    "\n",
    "## make diagnosis into a list for each row\n",
    "liste = df['primary_diagnosis'].apply(lambda x: [s for s in x.split('\\n') if s.strip()] if pd.notna(x) else x)\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"primary diagnoses\" not in item.lower()])\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"primary diagnosis\" not in item.lower()])\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"primary\" not in item.lower()]) \n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"====\" not in item.lower()])\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"\" != item.lower()])\n",
    "\n",
    "import re\n",
    "def remove_number_prefix(item):\n",
    "    return re.sub(r'^[1-8]\\)\\s*', '', item)\n",
    "liste = liste.apply(lambda lst: [remove_number_prefix(item) for item in lst])\n",
    "\n",
    "df[\"primary_diagnosis\"] = liste\n",
    "\n",
    "\n",
    "df['secondary_diagnosis'] = df['secondary_diagnosis'].fillna(\"\")\n",
    "liste = df['secondary_diagnosis'].apply(lambda x: [s for s in x.split('\\n') if s.strip()])\n",
    "\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"secondary diagnoses\" not in item.lower()])\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"secondary diagnosis\" not in item.lower()])\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"secondary\" not in item.lower()]) \n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"====\" not in item.lower()])\n",
    "liste = liste.apply(lambda lst: [item for item in lst if \"\" != item.lower()])\n",
    "\n",
    "import re\n",
    "def remove_number_prefix(item):\n",
    "    return re.sub(r'^[1-8]\\)\\s*', '', item)\n",
    "liste = liste.apply(lambda lst: [remove_number_prefix(item) for item in lst])\n",
    "\n",
    "df[\"secondary_diagnosis\"] = liste"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envphd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
